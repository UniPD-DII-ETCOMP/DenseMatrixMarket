%% Computing matrix exponential of banded matrices

%% Approximation with banded matrices
% It is known that the matrix exponential of banded matrix enjoy a decay
% property in the entries, as we move far from the diagonal. That is,
%
% $$ |[e^{A}]_{ij}| \leq C \rho^{|j - i|},  \qquad i,j = 1, \ldots, n $$
%
% where the constant $C$ and the rate $\rho$ depend on how well the
% exponential is well approximated by polynomials on the spectrum of A (if
% A is normal), or on its field of values (for the general case). Indeed,
% this result holds for a generic matrix function $f(A)$.
%
% The decay property might deteroriate if $A$ has a large norm, for
% instance when it is obtained by discretizing unbounded differential
% operators.

%% The matrix under consideration
% As an example, we consider the discretization of the 1D
% Laplacian
%
% $$ \displaystyle
%  A := -\frac{1}{h^2} \left[ \begin{array}{cccc}
%    2  & -1 &  & \\
%    -1 &  2 & \ddots & \\
%    & \ddots & \ddots & -1 \\
%    & & -1 & 2 \\
%  \end{array} \right], \qquad
%  h := \frac{1}{n - 1}
% $$
%
% This matrix has spectral norm that grows as $n^2$, and we assume that
% we would like to compute $e^{A}$.

%% Using HODLR and HSS arithmetic
% Even though, as we will see, the approximate sparse structure is quickly
% lost for large norms $\| A \|$, the matrix $e^A$ has a
% hierarchical rank structure that can be efficiently exploited using HSS
% or HODLR arithmetic.
%
% HM-TOOLBOX provides a routine EXPM both for the HSS and HODLR classes,
% and this implements a Pad√® approximant coupled with a scaling and
% squaring strategy, similarly to the MATLAB function EXPM for dense
% matrices.
%
% We now compute $e^A$ for different choices of $n$. This can be achieved
% in the toolbox as follows. First, let us choose a reasonably large value
% of $n$, but not too large so we can compare with the dense EXPM.

n = 2048; h = 1 / (n - 1);

%%
% Then, we can define the matrix $A$ as a banded matrix, and using the
% banded constructore in HM-TOOLBOX to obtained an HSS and HODLR
% representation.

sA = -h^2 \ spdiags(ones(n,1) * [-1 2 -1], -1:1, n, n);
hodlrA = hodlr('banded', sA);
hssA = hss('banded', sA);

%%
% The toolbox automatically determined the bandwidth of the matrix by
% calling bandwidth, but we could have provided it in case we know it. See
% ''help hm'' and help hss for further details.
%
% As usual, the ranks in the offdiagonal blocks can be inspected with a
% call to the SPY command.

spy(hodlrA);


%%
% We can now re-compute the matrix exponential by calling EXPM

hodlr_expA = expm(hodlrA);
hss_expA   = expm(hssA);


%%
% Let us now check the accuracy by comparing with the dense computation.

eA = expm(full(sA));
res = norm(eA - hodlr_expA)

%%
% The result is accurate almost up to the default truncation tolerance
% ($10^{-12}$), but the norm of $e^A$ is rather small, since $A$ is
% negative definite, and it $e^{\tau A}$ goes to zero for large enough $\tau$.
%
% For a more interesting case, we may consider the exponential of $\tau A$,
% for some $\tau < 1$. For intstance, we may choose $\tau = 1/100$.

tau = 1 / 100;

%%
% We can now re-compute the matrix exponential by calling EXPM

hodlr_expA = expm(tau * hodlrA);
hss_expA   = expm(tau * hssA);

%%
% And we may inspect the off-diagonal ranks of the result. This time, we
% try with the HSS one.

spy(hss_expA);

%%
% Note that the ranks have increased, but are still moderate, and indeed
% the HSS and HODLR storage are quite effective for this problem.
%
% Again, we check the residual by comparing with the dense solver.

eA = expm(tau * full(sA));
res = norm(eA - hss_expA)

%% Sparse vs HODLR / HSS storage
%
% Now that we have compute the matrix exponential, the toolbox allows to
% convert these matrices to sparse ones by thresholding the entries; we can
% try, for instance, to obtain an approximation to $e^A$ accurate to 8
% digits by running:

sp_expA = sparse(hss_expA, 1e-8);

%%
% We may check the bandwidth to see if the resulting matrix is sparse; the
% theory predicts the decay in the entries, but it gets slower as the norm
% increases.

bandwidth(sp_expA)

%%
% We immediately see that the bandwidth is almost equal to $n$, and
% therefore the sparse storage does not give any advantage for this case.
%
% Indeed, we can now compute the accuracy and the memory usage of this banded
% approximation, in Megabytes, and we compare it with the memory used by
% the HSS approximation (an analogous result would hold for the HODLR one).

mem_sparse = whos('sp_expA'); mem_sparse = mem_sparse.bytes / 1024^2
mem_hss    = whos('hss_expA'); mem_hss = mem_hss.bytes / 1024^2
res_sparse = norm(full(sp_expA) - eA)

%%
% On this example, the HSS wins both for storage and accuracy, since the
% residual for the rank structured approximation was order of magnitude
% slower than this one. Such gain would be even stronger as we increase $n$.
